# -*- coding: utf-8 -*-
"""BigDataProject_Sarthak_Gupta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MFJcNFC6mHktObiwiP474BXL--c3x0W7
"""

!nvidia-smi

!pip install clean-text --quiet
!pip install transformers --quiet
!pip install torch --quiet
!pip install tokenizer --quiet
!pip install sentencepiece --quiet
!pip install pytorch-lightning --quiet

import argparse
import glob
import os
import json
import time
import logging
import random 
import re
from itertools import chain
from string import punctuation
import pandas as pd
import numpy as np
import torch
import pathlib as Path
from torch.utils.data import Dataset, DataLoader
from cleantext import clean
print(torch.__version__)

import pytorch_lightning as pl
from sklearn.model_selection import train_test_split
from termcolor import colored

import textwrap

from transformers import (
    
    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup


)
from tqdm.auto import tqdm

pl.seed_everything(42) #to change

data = pd.read_csv("https://raw.githubusercontent.com/marcoguerini/CONAN/master/Multitarget-CONAN/Multitarget-CONAN.csv", encoding='latin-1')
data.head()

data = data[['HATE_SPEECH','COUNTER_NARRATIVE','TARGET']]

data.columns = ['hate_speech','counter_speech','context']

def pre_processing(line):
  lineS = line.lower()
  lineS = clean(lineS, no_emoji=True)
  return lineS

data_to_use = data.copy()
data_to_use['hate_speech'] = data_to_use['hate_speech'].apply(pre_processing)
data_to_use['counter_speech'] = data_to_use['counter_speech'].apply(pre_processing)
data_to_use.head()

"""## Tokenization"""

MODEL_NAME = "t5-large"

tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

def average_characters(data_frame):

  hate_list = []
  counter_list = []
  hateSpeech_len, counterSpeech_len = 0, 0
  for indx in range(data_frame.shape[0]):
    hateSpeech_len += len(data_frame.iloc[indx]['hate_speech'].split())
    hate_list.append(len(data_frame.iloc[indx]['hate_speech'].split()))
    counterSpeech_len += len(data_frame.iloc[indx]['counter_speech'].split())
    counter_list.append(len(data_frame.iloc[indx]['counter_speech'].split()))

  hate_speech_avg_len = hateSpeech_len//data_frame.shape[0]
  counter_speech_avg_len = counterSpeech_len//data_frame.shape[0]

  return hate_speech_avg_len,counter_speech_avg_len, hate_list, counter_list

question_avg_len, answer_avg_len, hate_list, counter_list = average_characters(data_to_use)

# PLOT for Hate_Speech histogram

import matplotlib.pyplot as plt
data = hate_list
plt.hist(data, bins=10)
plt.show()

# PLOT for Counter_Speech histogram

data = counter_list
plt.hist(data, bins=10)
plt.show()

print(question_avg_len, answer_avg_len)

class CounterSpeechDataset(Dataset):

  def __init__(
      self,
      data: pd.DataFrame,
      tokenizer: T5Tokenizer,
      source_max_token_len: int = 30,
      target_max_token_len: int = 60
  ):
    self.tokenizer = tokenizer
    self.data = data
    self.source_max_token_len = source_max_token_len
    self.target_max_token_len = target_max_token_len

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index: int):
    data_row = self.data.iloc[index]

    source_encoding = tokenizer(
      data_row["hate_speech"],
      data_row["context"],
      max_length = self.source_max_token_len,
      padding="max_length",
      truncation = "only_first",
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors="pt"
    )

    target_encoding = tokenizer(
      data_row["counter_speech"],
      max_length = self.target_max_token_len,
      padding="max_length",
      truncation = True,
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors="pt"
  )
    
    labels = target_encoding["input_ids"]
    labels[labels == 0] = -100

    return dict(
        hate_speech = data_row["hate_speech"],
        context = data_row["context"],
        counter_speech = data_row["counter_speech"],
        input_ids = source_encoding["input_ids"].flatten(),
        attention_mask = source_encoding["attention_mask"].flatten(),
        labels = labels.flatten()
    )

sample_dataset = CounterSpeechDataset(data_to_use, tokenizer)
for data in sample_dataset:
  print(data["hate_speech"])
  print(data["counter_speech"])
  print(data["input_ids"][:10])
  print(data["labels"][:10])
  break

data_to_train, data_to_test = data_to_use[:4500], data_to_use[4500:]

train_df, val_df = train_test_split(data_to_train, test_size = 0.15)

train_df.shape, val_df.shape

class CounterSpeechGeneratorModule(pl.LightningDataModule):

  def __init__(
      self,
      train_df : pd.DataFrame,
      test_df : pd.DataFrame,
      tokenizer: T5Tokenizer,
      batch_size: int = 3,
      source_max_token_len: int = 30,
      target_max_token_len: int = 60
  ):
    super().__init__()
    self.batch_size = batch_size
    self.train_df = train_df
    self.test_df = test_df
    self.tokenizer = tokenizer
    self.source_max_token_len = source_max_token_len
    self.target_max_token_len = target_max_token_len

  def setup(self, stage=None):
    self.train_dataset = CounterSpeechDataset(
        self.train_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

    self.test_dataset = CounterSpeechDataset(
        self.test_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

  def train_dataloader(self):
    return DataLoader(
        self.train_dataset,
        batch_size=self.batch_size,
        shuffle=True,
        num_workers = 6
    )

  def val_dataloader(self):
    return DataLoader(
        self.test_dataset,
        batch_size=1,
        num_workers = 6
    )

  def test_dataloader(self):
    return DataLoader(
        self.test_dataset,
        batch_size=1,
        num_workers = 6
    )

BATCH_SIZE = 20
N_EPOCHS = 5

data_module = CounterSpeechGeneratorModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)
data_module.setup()

model =  T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

model.config

"""## Modelling

"""

class CounterSpeechTrainingModel(pl.LightningModule):

  def __init__(self):
    super().__init__()
    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

  def forward(self, input_ids, attention_mask, labels=None):
    output = self.model(
        input_ids=input_ids,
        attention_mask = attention_mask,
        labels = labels
    )
    return output.loss, output.logits

  def training_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("train_loss", loss, prog_bar=True, logger=True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("test_loss", loss, prog_bar=True, logger=True)
    return loss

  def validation_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("val_loss", loss, prog_bar=True, logger=True)
    return loss

  def configure_optimizers(self):
    return AdamW(self.parameters(), lr = 0.0001)

model = CounterSpeechTrainingModel()

from pytorch_lightning.callbacks import ModelCheckpoint
checkpoint_callbacks = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

from pytorch_lightning.loggers import TensorBoardLogger
logger = TensorBoardLogger('lightning_logs',name = 'Counter-Speech-Generator')

trainer = pl.Trainer(
    callbacks = [checkpoint_callbacks],
    max_epochs = N_EPOCHS,
    devices = 1,
    accelerator="gpu",
    enable_progress_bar=True 
)

#!kill -9 6749

#!ps -ef | grep tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard 
# %tensorboard --logdir ./lightning_logs

trainer.fit(model, data_module)

trained_model = CounterSpeechTrainingModel.load_from_checkpoint('/content/checkpoints/best-checkpoint-v1.ckpt')
trained_model.freeze()

sample_data = data_to_test.iloc[15]

sample_data['hate_speech']

sample_data['counter_speech']

def generate_answer(sample=sample_data):
  source_encoding = tokenizer(
      sample['hate_speech'],
      sample['context'],
      max_length=30,
      padding='max_length',
      truncation='only_first',
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors='pt'
  )
  source_encoding.to('cuda')
  generated_ids = trained_model.model.generate(
      input_ids = source_encoding['input_ids'],
      attention_mask=source_encoding['attention_mask'],
      # num_beams=1,
      max_length=40,
      repetition_penalty=2.5,
      length_penalty=1.0,
      early_stopping=True,
      use_cache=True
  )
  
  preds = [
      tokenizer.decode(generate_id,skip_special_tokens=True,clean_up_tokenization_spaces=True)
      for generate_id in generated_ids
  ]
  
  return "".join(preds).encode('utf-8').strip()

output = generate_answer(sample_data)

for indx in range(len(data_to_test)):
  with open('test.txt','a') as f:
    f.write('\n HATE_SPEECH = ')
    f.write(data_to_test.iloc[indx]['hate_speech'])
    f.write('\n GIVEN_COUNTER = ')
    f.write(data_to_test.iloc[indx]['counter_speech'])
    f.write('\n GENERATED = ')
    output = generate_answer(data_to_test.iloc[indx])
    #print(str(output).strip('b\''))
    f.write(str(output).strip('b\''))
    f.write('\n')

for indx in range(len(data_to_test)):
  with open('test.txt','a') as f:
    #f.write('\n HATE_SPEECH = ')
    f.write(data_to_test.iloc[indx]['hate_speech'])
    f.write('#')
    f.write(data_to_test.iloc[indx]['counter_speech'])
    f.write('#')
    output = generate_answer(data_to_test.iloc[indx])
    #print(str(output).strip('b\''))
    f.write(str(output).strip('b\''))
    f.write('\n')

